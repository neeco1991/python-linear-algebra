{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e006a250",
   "metadata": {},
   "source": [
    "# Eigenvectors and eigenvalues\n",
    "\n",
    "For most vectors, the transformation has the property to change the direction of the vector, but not for all.\n",
    "The eigenvectors of a transformation are the vectors that, after the transformation, don't change direction. The eigenvalues are scalars representing the scale factor of the eigenvectors during such transformation.\n",
    "\n",
    "Linearity of the transformation implies that every vector with the direction of the eigenvector are squished or stretched by the same amount: the eigenvalue of that eigenvector.\n",
    "\n",
    "If we consider a 3D rotation, what will be the vector that remains on its direction during the transformation? Of course: is the axis of the rotation!\n",
    "\n",
    "Much of the information we can retrieve about a transformation strongly depends on the coordinate system we are choosing, but this is not true for eigenvector and eigenvalues.\n",
    "\n",
    "## How do we find them\n",
    "\n",
    "An eigenvector $\\vec{v}$ for a transformation $A$, as for what we said, is a vector such that:\n",
    "\n",
    "$$\n",
    "A\\vec{v} = \\lambda \\vec{v}\n",
    "$$\n",
    "\n",
    "We can work a bit our equation noting that $\\lambda \\vec{v} = (\\lambda I)\\vec{v}$, where $I$ is the identity matrix:\n",
    "\n",
    "$$\n",
    "(A - I\\lambda)\\vec{v} = 0\n",
    "$$\n",
    "\n",
    "When this equation has a solution? Of course we have the trivial solution $\\vec{v} = 0$, but we are searching non-trivial solutions. Since $A - I\\lambda$ can be seen as a transformation, and since we know that a transformation of a vector can land on $0$ if and only if the transformation is squishing the space into a lower dimension, we are searching for a solution in which the determinant of such transformation is null:\n",
    "\n",
    "$$\n",
    "\\det(A - I\\lambda) = 0\n",
    "$$\n",
    "\n",
    "Remember, when we find the values of $\\lambda$ that satisfy the previous equation, we have found the $\\lambda$ such that $A\\vec{v} = \\lambda \\vec{v}$.\n",
    "\n",
    "## Considerations\n",
    "\n",
    "Not every transformation has eigenvectors, for instance rotations in the Euclidian space does not have them.\n",
    "\n",
    "We can have only one eigenvalue that has many eigenvectors. For instance a transformation that scale everything by some factor like this:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Has only one eigenvalue $\\lambda = 2$, but every vector in the Euclidian space is an eigenvector.\n",
    "\n",
    "## Eigenbasis\n",
    "\n",
    "**def:** A *diagonal matrix* is a matrix which has $0$ everywhere except on the diagonal.\n",
    "\n",
    "We can interpret a *diagonal matrix* as a matrix that has its eigenvectors on its diagonal.\n",
    "\n",
    "Diagonal matrices are such that:\n",
    "\n",
    "$$\n",
    "D^{n} = \\begin{pmatrix} a^n & 0 \\\\ 0 & b^n \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So, it is extremely easy to compute the *n-power* of them, while it is extremely complex with a random matrix.\n",
    "\n",
    "If our transformation has at least 2 eigenvectors, we can change our basis in a way that has the eigenvectors as basis vectors. If we change our basis to have the eigenbasis, we are guaranteed to have a diagonal matrix for transformation.\n",
    "\n",
    "Unfortunately, not all transformation have 2 eigenvectors. For instance, a shear have only one eigenvector."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
