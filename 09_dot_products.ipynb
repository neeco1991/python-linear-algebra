{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dedcb05",
   "metadata": {},
   "source": [
    "# Dot products\n",
    "\n",
    "Numerically, doing the dot product between two vectors *of the same size* means to multiply each coordinate of the two vectors and sum them up:\n",
    "\n",
    "$$\n",
    "\\vec{v}*\\vec{w} = \\begin{pmatrix} 2 \\\\ 7 \\\\ 1 \\end{pmatrix} * \\begin{pmatrix} 8 \\\\ 2 \\\\ 8 \\end{pmatrix} = 2*8 + 7*2 + 1*8 = 38\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([2, 7, 1])\n",
    "w = np.array([8, 2, 8])\n",
    "\n",
    "dot_product = np.dot(v, w)\n",
    "\n",
    "print(f\"The vector v is: {v}\")\n",
    "print(f\"The vector w is: {w}\")\n",
    "print(f\"The dot product of v and w is: {dot_product}\")\n",
    "\n",
    "manual_dot_product = np.sum(v * w)\n",
    "print(f\"Manual calculation (v * w).sum() gives: {manual_dot_product}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115c678",
   "metadata": {},
   "source": [
    "### Geometric interpretation\n",
    "\n",
    "Making the dot product between two vectors is the same of projecting one vector onto the other, and the to multiply their lenght. When the two vectors has opposite direction, the dot product will be negative. When they have the same direction, the dot product will be positive. Finally, when the dot product is null, then the two vector are perpedicolar.\n",
    "\n",
    "Order does not matter: we can project the first vector onto the second and then multiply the lenght, or do the exact opposite: project the second vector onto the first and then multiply the lenghts. Intuitively, this is true when the two vectors has the same size because of simmetry. If we scale the first vector by $2$, then the dot product become:\n",
    "\n",
    "$$\n",
    "(2\\vec{v})*\\vec{w} = 2(\\vec{v}*\\vec{w})\n",
    "$$\n",
    "\n",
    "This is because the lenght of $\\vec{w}$ projected onto $\\vec{v}$ doesn't change, since the direction of $\\vec{v}$ is the same. On the other hand, if we project $2\\vec{v}$ onto $\\vec{w}$ we have that the lenght of the projection is doubled, but $\\vec{w}$ remain constant. \n",
    "\n",
    "### Linear transformations\n",
    "\n",
    "Consider a linear that transforms every vector in $\\real^2$ into a number in $\\real$. The transformation matrix will be a $2 * 1$ matrix which has where $\\hat{i}$ and $\\hat{j}$ lands has columns. Suppose we have this linear transformation $L$:\n",
    "\n",
    "$$\n",
    "L = \\begin{pmatrix} 1 & -2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and we want to apply it and check where the vector $\\vec{v} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}$ end up be after being transformed.\n",
    "\n",
    "$$\n",
    "L(\\vec{v}) = \\begin{pmatrix} 1 & -2 \\end{pmatrix} * \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} = 1 * 4 + (-2) * 3 = -2\n",
    "$$\n",
    "\n",
    "Which is $4$ times where $\\hat{i}$ plus $3$ times where $\\hat{j}$ lands.\n",
    "\n",
    "But this really looks like taking a dot product with the vector $\\vec{v}$ transposed. \n",
    "\n",
    "Let be $L$ a linear transformation from $\\real^2$ to $\\real$ that project every point in the space, onto a straight line that crosses the origin. This transformation is linear, in fact, sequences of equally spaced dot in $\\real^2$ will be equally spaced on this line. Let be $\\hat{u}$ the versor in the input space that has the direction of this said line. When we project $\\hat{i}$ onto the $\\hat{u}$ direction, we get the same number $\\hat{u_x}$ as we would get if we project $\\hat{u}$ onto $\\hat{i}$, and the same is obvioulsy true for $\\hat{j}$. This means that our transformation is $L = \\begin{pmatrix} \\hat{u_x} & \\hat{u_y} \\end{pmatrix}$.\n",
    "\n",
    "Hence, when we apply this linear transformation to a point we have:\n",
    "\n",
    "$$\n",
    "L*\\vec{x} = \\begin{pmatrix} \\hat{u_x} & \\hat{u_y} \\end{pmatrix} * \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\hat{u_x}*x + \\hat{u_y}*y\n",
    "$$\n",
    "\n",
    "Therefore, taking the dot product between two vector is mathematically the same as applying a linear transformation using the first as the transformation matrix.\n",
    "\n",
    "So, everytime we see a 2D to 1D linear transformation as a $1 * 2$ matrix, we can think of it as projecting all the Euclidian space onto the line that has the same direction as the transformation matrix transposed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-linear-algebra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
